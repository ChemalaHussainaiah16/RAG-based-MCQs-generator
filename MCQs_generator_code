import streamlit as st
import os
from dotenv import load_dotenv
import fitz  
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langchain.chains import RetrievalQA


load_dotenv()


st.set_page_config(page_title="GenAI PDF QA with AzureOpenAI", layout="wide")

st.title("üìÑ Ask Questions from Your GenAI PDF using Azure OpenAI")


uploaded_file = st.file_uploader("Upload a GenAI-related PDF", type="pdf")

if uploaded_file is not None:
    # Step 1: Extract text
    with fitz.open(stream=uploaded_file.read(), filetype="pdf") as doc:
        text = ""
        for page in doc:
            text += page.get_text()

    if text.strip() == "":
        st.error("‚ùå The uploaded PDF has no extractable text.")
        st.stop()

    
    splitter = CharacterTextSplitter(separator="\n", chunk_size=1000, chunk_overlap=200)
    chunks = splitter.split_text(text)

    
    embeddings = AzureOpenAIEmbeddings(
        azure_endpoint=os.getenv("EMBEDDING_AZURE_OPENAI_ENDPOINT"),
        api_key=os.getenv("EMBEDDING_AZURE_OPENAI_API_KEY"),
        api_version=os.getenv("EMBEDDING_AZURE_OPENAI_API_VERSION"),
        deployment=os.getenv("EMBEDDING_AZURE_OPENAI_DEPLOYMENT_NAME")
    )

  
    vectorstore = FAISS.from_texts(chunks, embeddings)

    
    llm = AzureChatOpenAI(
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
        deployment_name=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
        temperature=0
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        return_source_documents=True
    )

   
    question = st.text_input("üí¨ Ask a question from the PDF content")

    if question:
        with st.spinner("Generating answer..."):
            result = qa_chain({"query": question})
            st.subheader("üîç Answer:")
            st.write(result["result"])

    
    st.subheader("üß† Generate Questions from PDF")

    if st.button("Generate 5 MCQs and 5 Short Answer Questions"):
        prompt = (
            "You are a quiz generator. Based on the following text, create:\n"
            "- 5 Multiple Choice Questions (MCQs) with 4 options each and mark the correct answer.\n"
            "- 5 Short Answer Questions with concise answers.\n\n"
            f"Text:\n{text[:3000]}"  #
        )
        with st.spinner("Generating questions..."):
            response = llm.predict(prompt)
            st.text_area("üìã Questions & Answers", response, height=400)
else:
    st.info("üëÜ Upload a PDF to get started.")
